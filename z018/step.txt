git clone https://github.com/Zerolone/pythonstudy.git

scrapy startproject z018

cd z018

scrapy genspider z1 z.com

scrapy crawl z1
scrapy crawl z1 -o sss_z1.json -t json

--nolog不显示日志
scrapy crawl z1 --nolog -a ddid=1



pip install scrapyd

创建egg
https://www.cnblogs.com/kungfupanda/p/3343113.html

到z018目录， 运行
cd ~/Desktop/pythonstudy
cd z018
python setup.py bdist_egg
cd dist
就可以了。
当然， 可以相应修改setup.py 文件

curl http://localhost:6800/addversion.json -F project=z018 -F version=r001 -F egg=@project-1.0-py2.7.egg



curl http://localhost:6800/schedule.json -d project=z018 -d spider=z1 -d setting=DOWNLOAD_DELAY=2 -d ddid=1
curl http://localhost:6800/schedule.json -d project=z018 -d spider=z1 -d setting=DOWNLOAD_DELAY=2 -d ddid=2
curl http://localhost:6800/schedule.json -d project=z018 -d spider=z1 -d setting=DOWNLOAD_DELAY=2 -d ddid=3
curl http://localhost:6800/schedule.json -d project=z018 -d spider=z1 -d setting=DOWNLOAD_DELAY=2 -d ddid=4
curl http://localhost:6800/schedule.json -d project=z018 -d spider=z1 -d setting=DOWNLOAD_DELAY=2 -d ddid=5
curl http://localhost:6800/schedule.json -d project=z018 -d spider=z1 -d setting=DOWNLOAD_DELAY=2 -d ddid=6
curl http://localhost:6800/schedule.json -d project=z018 -d spider=z1 -d setting=DOWNLOAD_DELAY=2 -d ddid=7
curl http://localhost:6800/schedule.json -d project=z018 -d spider=z1 -d setting=DOWNLOAD_DELAY=2 -d ddid=8
curl http://localhost:6800/schedule.json -d project=z018 -d spider=z1 -d setting=DOWNLOAD_DELAY=2 -d ddid=9
curl http://localhost:6800/schedule.json -d project=z018 -d spider=z1 -d setting=DOWNLOAD_DELAY=2 -d ddid=10
curl http://localhost:6800/schedule.json -d project=z018 -d spider=z1 -d setting=DOWNLOAD_DELAY=2 -d ddid=11

#终止爬虫
curl http://192.168.2.239:6800/cancel.json -d project=Crawler -d job= 8270364f9d9811e5adbf000c29a5d5be


scrapyd-deploy scrapyd_z018 -p z018

easy_install
mysql-python
apt-get install libmariadbclient-dev

pycurl
apt-get install libssl-dev libcurl4-openssl-dev python-dev


http://scrapyd.readthedocs.io/en/latest/config.html


1、建立一个采集任务， 接受参数为ddid，功能为获取相应的地址， 然后获取评论内容，
将评论数据采集，通过接口将数据入库，入库规则可以在接口中实现。

2、通过scrapyd管理每个采集任务进行采集

3、接口增加一个功能，获取当前采集任务数量，提交到scrapyd，增加采集任务。 可以根据时间来临时分配
